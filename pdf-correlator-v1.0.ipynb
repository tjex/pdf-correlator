{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717c71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf-correlator by Tillman Jex\n",
    "# github.com/tjex\n",
    "# tillmanjex@mailbox.org\n",
    "\n",
    "import os, glob, re, io, random, gensim, smart_open, logging, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as pdfminer_extraction\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pdfReaders = []\n",
    "pdfFiles = []\n",
    "docLabels = []\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootDir = \"/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored\"\n",
    "txtExtractDir = rootDir + '/txt-extractions'\n",
    "modelDataDir = rootDir + '/model-data'\n",
    "testsDir = '/Users/tillman/t-root/dev/projects/2022/pdf-correlator/tests'\n",
    "zoteroDir = '/Users/tillman/t-root/zotero/storage'\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980957ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PART 1 - READ, EXTRACT, TRAIN AND ASSESS ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba894b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pdfs in/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored (including subdirectories)\n",
      "\u001b[91merror: pdf-tests/subdirectory/Simeone et al_2018_Arts and design as translational mechanisms for academic entrepreneurship.pdf is unreadable by glob.glob. Skipping file\u001b[0m\n",
      "\u001b[92mpdf files read\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "print(\"reading pdfs in\" + str(rootDir) + \" (including subdirectories)\")\n",
    "def read_files():\n",
    "    os.chdir(rootDir)\n",
    "    for file in glob.glob(\"**/*.pdf\", recursive=True):\n",
    "        try:\n",
    "            pdfFiles.append(file)\n",
    "            pdfReaders.append(PdfReader(file))\n",
    "        except:\n",
    "            print(bcolors.FAIL + \"error: \" + file + \" is unreadable by glob.glob. Skipping file\" + bcolors.ENDC)\n",
    "    print(bcolors.OKGREEN + \"pdf files read\" + bcolors.ENDC)\n",
    "    print()\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ffcb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pdfs to text files (duplicate pdfs are handled automagically)\n",
      "\u001b[91merror: \"None\" not extractable with PyPDF2, trying with pdfminer\u001b[0m\n",
      "\n",
      "\u001b[92mpdf extraction complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# extract text from pdfs to designated directory and save as txt files.\n",
    "def extract_to_txt():\n",
    "    print(\"Extracting pdfs to text files (duplicate pdfs are handled automagically)\")\n",
    "    os.chdir(txtExtractDir)\n",
    "    counter = 0\n",
    "    text = \"\"\n",
    "    for i in pdfReaders:\n",
    "        counter += 1\n",
    "        with open(str([i.metadata.title]) + \".txt\", 'w', encoding=\"utf-8\") as file:\n",
    "      \n",
    "            # add doc title to array for reference / tagging\n",
    "            docLabels.append(i.metadata.title)\n",
    "            try:\n",
    "                for j in range(len(i.pages)):\n",
    "                    # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                    text += i.getPage(j).extract_text()\n",
    "                    text = \"\".join(line.strip(\"\\n\") for line in text)  \n",
    "\n",
    "                \n",
    "                    \n",
    "            except Exception as exc:\n",
    "                print(bcolors.FAIL + \"error: \" + \"\\\"\" + str(i.metadata.title) + \"\\\"\" + \" not extractable with PyPDF2, trying with pdfminer\" + bcolors.ENDC)\n",
    "                print()\n",
    "                # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                text += pdfminer_extraction(rootDir + \"/\" + pdfFiles[counter])\n",
    "                text = \"\".join(line.strip(\"\\n\") for line in text)     \n",
    "                \n",
    " \n",
    "            file.write(text)\n",
    "    print(bcolors.OKGREEN + \"pdf extraction complete\" + bcolors.ENDC)\n",
    "extract_to_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1264dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a training corpus from all txt files found in designated directory\n",
    "class CorpusGen(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self, tokens_only=False):\n",
    "        counter = 0\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            \n",
    "            with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    tokens = gensim.utils.simple_preprocess(line, min_len=3, max_len=20, deacc=True)\n",
    "                    if tokens_only:\n",
    "                        yield tokens\n",
    "                    else:\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(tokens, [counter])\n",
    "            counter += 1\n",
    "        \n",
    "trainCorpus = list(CorpusGen('/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored/txt-extractions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4494fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire corpus to a txt file\n",
    "with open(modelDataDir + \"/train-corpus.txt\", 'w') as file:\n",
    "    file.write(str(trainCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d4ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 10:33:44,637 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>', 'datetime': '2022-09-30T10:33:44.637837', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2022-09-30 10:33:44,640 : INFO : collecting all words and their counts\n",
      "2022-09-30 10:33:44,641 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-09-30 10:33:44,723 : INFO : collected 13122 word types and 12 unique tags from a corpus of 12 examples and 669596 words\n",
      "2022-09-30 10:33:44,723 : INFO : Creating a fresh vocabulary\n",
      "2022-09-30 10:33:44,744 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 12698 unique words (96.77% of original 13122, drops 424)', 'datetime': '2022-09-30T10:33:44.744148', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-30 10:33:44,744 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 669172 word corpus (99.94% of original 669596, drops 424)', 'datetime': '2022-09-30T10:33:44.744588', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-30 10:33:44,771 : INFO : deleting the raw counts dictionary of 13122 items\n",
      "2022-09-30 10:33:44,771 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2022-09-30 10:33:44,772 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 579202.6031504513 word corpus (86.6%% of prior 669172)', 'datetime': '2022-09-30T10:33:44.772195', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-30 10:33:44,817 : INFO : estimated required memory for 12698 words and 50 dimensions: 11433000 bytes\n",
      "2022-09-30 10:33:44,818 : INFO : resetting layer weights\n",
      "2022-09-30 10:33:44,821 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 12698 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-09-30T10:33:44.821166', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-09-30 10:33:44,874 : INFO : EPOCH 0: training on 669596 raw words (104555 effective words) took 0.1s, 1993248 effective words/s\n",
      "2022-09-30 10:33:44,927 : INFO : EPOCH 1: training on 669596 raw words (104535 effective words) took 0.1s, 2080697 effective words/s\n",
      "2022-09-30 10:33:44,981 : INFO : EPOCH 2: training on 669596 raw words (104523 effective words) took 0.1s, 1986588 effective words/s\n",
      "2022-09-30 10:33:45,037 : INFO : EPOCH 3: training on 669596 raw words (104532 effective words) took 0.1s, 1944078 effective words/s\n",
      "2022-09-30 10:33:45,089 : INFO : EPOCH 4: training on 669596 raw words (104578 effective words) took 0.0s, 2098611 effective words/s\n",
      "2022-09-30 10:33:45,144 : INFO : EPOCH 5: training on 669596 raw words (104549 effective words) took 0.1s, 1994143 effective words/s\n",
      "2022-09-30 10:33:45,197 : INFO : EPOCH 6: training on 669596 raw words (104576 effective words) took 0.1s, 2074092 effective words/s\n",
      "2022-09-30 10:33:45,250 : INFO : EPOCH 7: training on 669596 raw words (104544 effective words) took 0.1s, 2053753 effective words/s\n",
      "2022-09-30 10:33:45,304 : INFO : EPOCH 8: training on 669596 raw words (104524 effective words) took 0.1s, 1979517 effective words/s\n",
      "2022-09-30 10:33:45,354 : INFO : EPOCH 9: training on 669596 raw words (104543 effective words) took 0.0s, 2138369 effective words/s\n",
      "2022-09-30 10:33:45,407 : INFO : EPOCH 10: training on 669596 raw words (104543 effective words) took 0.1s, 2037078 effective words/s\n",
      "2022-09-30 10:33:45,459 : INFO : EPOCH 11: training on 669596 raw words (104552 effective words) took 0.1s, 2038865 effective words/s\n",
      "2022-09-30 10:33:45,509 : INFO : EPOCH 12: training on 669596 raw words (104541 effective words) took 0.0s, 2163656 effective words/s\n",
      "2022-09-30 10:33:45,560 : INFO : EPOCH 13: training on 669596 raw words (104536 effective words) took 0.1s, 2074470 effective words/s\n",
      "2022-09-30 10:33:45,611 : INFO : EPOCH 14: training on 669596 raw words (104528 effective words) took 0.0s, 2160934 effective words/s\n",
      "2022-09-30 10:33:45,662 : INFO : EPOCH 15: training on 669596 raw words (104564 effective words) took 0.0s, 2101518 effective words/s\n",
      "2022-09-30 10:33:45,713 : INFO : EPOCH 16: training on 669596 raw words (104529 effective words) took 0.0s, 2098444 effective words/s\n",
      "2022-09-30 10:33:45,763 : INFO : EPOCH 17: training on 669596 raw words (104560 effective words) took 0.0s, 2192610 effective words/s\n",
      "2022-09-30 10:33:45,810 : INFO : EPOCH 18: training on 669596 raw words (104540 effective words) took 0.0s, 2310215 effective words/s\n",
      "2022-09-30 10:33:45,861 : INFO : EPOCH 19: training on 669596 raw words (104561 effective words) took 0.0s, 2100940 effective words/s\n",
      "2022-09-30 10:33:45,909 : INFO : EPOCH 20: training on 669596 raw words (104527 effective words) took 0.0s, 2236944 effective words/s\n",
      "2022-09-30 10:33:45,958 : INFO : EPOCH 21: training on 669596 raw words (104560 effective words) took 0.0s, 2213738 effective words/s\n",
      "2022-09-30 10:33:46,006 : INFO : EPOCH 22: training on 669596 raw words (104551 effective words) took 0.0s, 2231306 effective words/s\n",
      "2022-09-30 10:33:46,052 : INFO : EPOCH 23: training on 669596 raw words (104564 effective words) took 0.0s, 2340060 effective words/s\n",
      "2022-09-30 10:33:46,100 : INFO : EPOCH 24: training on 669596 raw words (104560 effective words) took 0.0s, 2233959 effective words/s\n",
      "2022-09-30 10:33:46,147 : INFO : EPOCH 25: training on 669596 raw words (104554 effective words) took 0.0s, 2362157 effective words/s\n",
      "2022-09-30 10:33:46,193 : INFO : EPOCH 26: training on 669596 raw words (104549 effective words) took 0.0s, 2277363 effective words/s\n",
      "2022-09-30 10:33:46,239 : INFO : EPOCH 27: training on 669596 raw words (104555 effective words) took 0.0s, 2324437 effective words/s\n",
      "2022-09-30 10:33:46,284 : INFO : EPOCH 28: training on 669596 raw words (104549 effective words) took 0.0s, 2541983 effective words/s\n",
      "2022-09-30 10:33:46,330 : INFO : EPOCH 29: training on 669596 raw words (104554 effective words) took 0.0s, 2312127 effective words/s\n",
      "2022-09-30 10:33:46,376 : INFO : EPOCH 30: training on 669596 raw words (104534 effective words) took 0.0s, 2322277 effective words/s\n",
      "2022-09-30 10:33:46,422 : INFO : EPOCH 31: training on 669596 raw words (104520 effective words) took 0.0s, 2347564 effective words/s\n",
      "2022-09-30 10:33:46,467 : INFO : EPOCH 32: training on 669596 raw words (104530 effective words) took 0.0s, 2430436 effective words/s\n",
      "2022-09-30 10:33:46,514 : INFO : EPOCH 33: training on 669596 raw words (104540 effective words) took 0.0s, 2344119 effective words/s\n",
      "2022-09-30 10:33:46,560 : INFO : EPOCH 34: training on 669596 raw words (104559 effective words) took 0.0s, 2393131 effective words/s\n",
      "2022-09-30 10:33:46,607 : INFO : EPOCH 35: training on 669596 raw words (104540 effective words) took 0.0s, 2341452 effective words/s\n",
      "2022-09-30 10:33:46,652 : INFO : EPOCH 36: training on 669596 raw words (104536 effective words) took 0.0s, 2452944 effective words/s\n",
      "2022-09-30 10:33:46,696 : INFO : EPOCH 37: training on 669596 raw words (104555 effective words) took 0.0s, 2458257 effective words/s\n",
      "2022-09-30 10:33:46,741 : INFO : EPOCH 38: training on 669596 raw words (104543 effective words) took 0.0s, 2428124 effective words/s\n",
      "2022-09-30 10:33:46,787 : INFO : EPOCH 39: training on 669596 raw words (104548 effective words) took 0.0s, 2342305 effective words/s\n",
      "2022-09-30 10:33:46,787 : INFO : Doc2Vec lifecycle event {'msg': 'training on 26783840 raw words (4181841 effective words) took 2.0s, 2126837 effective words/s', 'datetime': '2022-09-30T10:33:46.787746', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# establish a model and build the vocab\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(trainCorpus)\n",
    "model.train(trainCorpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefe6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 10:34:04,295 : WARNING : attribute doc_count not present in KeyedVectors<vector_size=50, 12 keys>; will store in internal index_to_key order\n",
      "2022-09-30 10:34:04,298 : INFO : storing 12x50 projection weights into doc_tensor.w2v\n",
      "2022-09-30 10:34:04,306 : INFO : running ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
      "2022-09-30 10:34:04,308 : INFO : loading projection weights from doc_tensor.w2v\n",
      "2022-09-30 10:34:04,310 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (12, 50) matrix of type float32 from doc_tensor.w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-09-30T10:34:04.310748', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2022-09-30 10:34:04,312 : INFO : 2D tensor file saved to pdf_plot_tensor.tsv\n",
      "2022-09-30 10:34:04,313 : INFO : Tensor metadata file saved to pdf_plot_metadata.tsv\n",
      "2022-09-30 10:34:04,313 : INFO : finished running word2vec2tensor.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAFOR Journal of Cultural Studies – Volume 6 – Issue 1 \n",
      "frvir-2022-779148 1..5\n",
      "ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users\n",
      "The effects of visual context and individual differences on perception and evaluation of modern art and graffiti art\n",
      "User attention and behaviour in virtual reality art encounter\n",
      "Microsoft Word - 48710116.DOC\n",
      "pone.0099019 1..8\n",
      "Making Art Therapy Virtual: Integrating Virtual Reality Into Art Therapy With Adolescents\n",
      "Microsoft Word - CHI2018_LucidDreaming_v5.docx\n",
      "g5grap.lo\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate and format data files for tensorboard visualisation\n",
    "os.chdir(modelDataDir)\n",
    "model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)\n",
    "%run ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
    "\n",
    "text = \"\"    \n",
    "with open('pdf_plot_metadata.tsv', 'w') as file:\n",
    "    file.write('title\\n')\n",
    "    for fname in os.listdir(txtExtractDir):\n",
    "        if fname.endswith('.txt'):\n",
    "            text = fname\n",
    "            text = re.sub('\\[\\'', '', text)\n",
    "            text = re.sub('\\'\\].txt', '', text)\n",
    "            text = re.sub('\\[', '', text)\n",
    "            text = re.sub('\\].txt', '', text)     \n",
    "            print(text)\n",
    "            file.write(\"%s\\n\" % text)\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958a3c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"internet\" appears this many times in corpus:\n",
      "{96}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word occurence check\n",
    "checkWord = \"internet\"\n",
    "print(\"\\\"\" + str(checkWord) + \"\\\"\" + \" appears this many times in corpus:\") \n",
    "print({model.wv.get_vecattr(checkWord, 'count')})\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b802f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessing the model (this can take a while)\n",
      "\u001b[92mmodel assessed\u001b[0m\n",
      "\n",
      "\u001b[94mmodel trained and assessed successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# assessing the model\n",
    "print(\"assessing the model (this can take a while)\")\n",
    "ranks = []\n",
    "secondRanks = []\n",
    "for doc_id in range(len(trainCorpus)):\n",
    "        inferredVector = model.infer_vector(trainCorpus[doc_id].words)\n",
    "        sims = model.dv.most_similar([inferredVector], topn=len(model.dv))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        secondRanks.append(sims[1])\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(bcolors.OKGREEN + \"model assessed\" + bcolors.ENDC)\n",
    "print()\n",
    "print(bcolors.OKBLUE + \"model trained and assessed successfully\" + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PART 2 - CHECK SIMILARITY BETWEEN CORPUS AND INCOMING DOCUMENT ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50cfb5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing latin pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import new document\n",
    "\n",
    "print('importing latin pdf')\n",
    "with smart_open.open(testsDir + '/similarity-test.txt', 'w') as test:\n",
    "    text = pdfminer_extraction(testsDir + '/german.pdf')\n",
    "    text = \"\".join(line.strip(\"\\n\") for line in text) \n",
    "    test.write(text)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540252ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag and tokenize pdf for doc2vec processing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize and tag new document\n",
    "print('tag and tokenize pdf for doc2vec processing')\n",
    "def read_text(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "                \n",
    "similarityTest = list(read_text(testsDir + '/similarity-test.txt'))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e40139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare similarity between incoming pdf and text corpus (the model)\n",
      "> Incoming pdf is 38% similar to corpus\n"
     ]
    }
   ],
   "source": [
    "# check for similarity against the entire corpus\n",
    "print('compare similarity between incoming pdf and text corpus (the model)')\n",
    "novel_vector = model.infer_vector(similarityTest[0].words)\n",
    "similarity_scores = model.dv.most_similar([novel_vector])\n",
    "average = 0\n",
    "for score in similarity_scores:\n",
    "    average += score[1]\n",
    "overall_similarity = average/len(similarity_scores)\n",
    "\n",
    "print(\"> Incoming pdf is \" + format((overall_similarity - 1) * -1, '.0%') + \" similar to corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee1e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

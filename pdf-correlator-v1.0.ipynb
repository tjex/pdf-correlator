{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf-correlator by Tillman Jex\n",
    "# github.com/tjex\n",
    "# tillmanjex@mailbox.org\n",
    "\n",
    "import os, glob, re, io, random, gensim, smart_open, logging, collections, sys\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as pdfminer_extraction\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "pdfReaders = []\n",
    "pdfFiles = []\n",
    "docLabels = []\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootDir = sys.path[0]\n",
    "pdfReadRootDir = rootDir + \"/pdf-tests\" # change this variable to where you would like to search for pdfs from\n",
    "txtExtractDir = rootDir + '/txt-extractions'\n",
    "modelDataDir = rootDir + '/model-data'\n",
    "testsDir = rootDir + '/tests'\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980957ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PART 1 - READ, EXTRACT, TRAIN AND ASSESS ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba894b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "print(\"reading pdfs in\" + str(rootDir) + \" (including subdirectories)\")\n",
    "def read_files():\n",
    "    os.chdir(pdfReadRootDir)\n",
    "    for file in glob.glob(\"**/*.pdf\", recursive=True):\n",
    "        try:\n",
    "            pdfFiles.append(file)\n",
    "            pdfReaders.append(PdfReader(file))\n",
    "        except:\n",
    "            print(bcolors.FAIL + \"error: \" + file + \" is unreadable by glob.glob. Skipping file\" + bcolors.ENDC)\n",
    "    print(bcolors.OKGREEN + \"pdf files read\" + bcolors.ENDC)\n",
    "    print()\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ffcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdfs to designated directory and save as txt files.\n",
    "def extract_to_txt():\n",
    "    print(\"Extracting pdfs to text files (duplicate pdfs are handled automagically)\")\n",
    "    os.chdir(txtExtractDir)\n",
    "    counter = 0\n",
    "    text = \"\"\n",
    "    for i in pdfReaders:\n",
    "        counter += 1\n",
    "        with open(str([i.metadata.title]) + \".txt\", 'w', encoding=\"utf-8\") as file:\n",
    "      \n",
    "            # add doc title to array for reference / tagging\n",
    "            docLabels.append(i.metadata.title)\n",
    "            try:\n",
    "                for j in range(len(i.pages)):\n",
    "                    # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                    text += i.getPage(j).extract_text()\n",
    "                    text = \"\".join(line.strip(\"\\n\") for line in text)  \n",
    "\n",
    "                \n",
    "                    \n",
    "            except Exception as exc:\n",
    "                print(bcolors.FAIL + \"error: \" + \"\\\"\" + str(i.metadata.title) + \"\\\"\" + \" not extractable with PyPDF2, trying with pdfminer\" + bcolors.ENDC)\n",
    "                print()\n",
    "                # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                text += pdfminer_extraction(rootDir + \"/\" + pdfFiles[counter])\n",
    "                text = \"\".join(line.strip(\"\\n\") for line in text)     \n",
    "                \n",
    " \n",
    "            file.write(text)\n",
    "    print(bcolors.OKGREEN + \"pdf extraction complete\" + bcolors.ENDC)\n",
    "extract_to_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a training corpus from all txt files found in designated directory\n",
    "class CorpusGen(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self, tokens_only=False):\n",
    "        counter = 0\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            \n",
    "            with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    tokens = gensim.utils.simple_preprocess(line, min_len=3, max_len=20, deacc=True)\n",
    "                    if tokens_only:\n",
    "                        yield tokens\n",
    "                    else:\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(tokens, [counter])\n",
    "            counter += 1\n",
    "        \n",
    "trainCorpus = list(CorpusGen(txtExtractDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4494fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire corpus to a txt file\n",
    "with open(modelDataDir + \"/train-corpus.txt\", 'w') as file:\n",
    "    file.write(str(trainCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a model and build the vocab\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(trainCorpus)\n",
    "model.train(trainCorpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and format data files for tensorboard visualisation\n",
    "os.chdir(modelDataDir)\n",
    "model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)\n",
    "%run ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
    "\n",
    "text = \"\"    \n",
    "with open('pdf_plot_metadata.tsv', 'w') as file:\n",
    "    file.write('title\\n')\n",
    "    for fname in os.listdir(txtExtractDir):\n",
    "        if fname.endswith('.txt'):\n",
    "            text = fname\n",
    "            text = re.sub('\\[\\'', '', text)\n",
    "            text = re.sub('\\'\\].txt', '', text)\n",
    "            text = re.sub('\\[', '', text)\n",
    "            text = re.sub('\\].txt', '', text)     \n",
    "            print(text)\n",
    "            file.write(\"%s\\n\" % text)\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word occurence check\n",
    "checkWord = \"the\"\n",
    "print(\"\\\"\" + str(checkWord) + \"\\\"\" + \" appears this many times in corpus:\") \n",
    "print({model.wv.get_vecattr(checkWord, 'count')})\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b802f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessing the model\n",
    "print(\"assessing the model (this can take a while)\")\n",
    "ranks = []\n",
    "secondRanks = []\n",
    "for doc_id in range(len(trainCorpus)):\n",
    "        inferredVector = model.infer_vector(trainCorpus[doc_id].words)\n",
    "        sims = model.dv.most_similar([inferredVector], topn=len(model.dv))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        secondRanks.append(sims[1])\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(bcolors.OKGREEN + \"model assessed\" + bcolors.ENDC)\n",
    "print()\n",
    "print(bcolors.OKBLUE + \"model trained and assessed successfully\" + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PART 2 - CHECK SIMILARITY BETWEEN CORPUS AND INCOMING DOCUMENT ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new document\n",
    "\n",
    "print('importing latin pdf')\n",
    "with smart_open.open(testsDir + '/similarity-test.txt', 'w') as test:\n",
    "    text = pdfminer_extraction(testsDir + '/german.pdf')\n",
    "    text = \"\".join(line.strip(\"\\n\") for line in text) \n",
    "    test.write(text)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540252ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and tag new document\n",
    "print('tag and tokenize pdf for doc2vec processing')\n",
    "def read_text(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "                \n",
    "similarityTest = list(read_text(testsDir + '/similarity-test.txt'))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e40139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for similarity against the entire corpus\n",
    "print('compare similarity between incoming pdf and text corpus (the model)')\n",
    "novel_vector = model.infer_vector(similarityTest[0].words)\n",
    "similarity_scores = model.dv.most_similar([novel_vector])\n",
    "average = 0\n",
    "for score in similarity_scores:\n",
    "    average += score[1]\n",
    "overall_similarity = average/len(similarity_scores)\n",
    "\n",
    "print(\"> Incoming pdf is \" + format((overall_similarity - 1) * -1, '.0%') + \" similar to corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee1e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

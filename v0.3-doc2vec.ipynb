{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717c71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os, glob, re, io, random, gensim, smart_open, logging, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as fallback_text_extraction\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pdfReaders = []\n",
    "pdfFiles = []\n",
    "docLabels = []\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootDir = \"/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored\"\n",
    "txtExtractDir = rootDir + '/txt-extractions'\n",
    "modelDataDir = rootDir + '/model-data'\n",
    "zoteroDir = '/Users/tillman/t-root/zotero/storage'\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba894b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pdfs in/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored (including subdirectories)\n",
      "\u001b[91merror: pdf-tests/Simeone et al_2018_Arts and design as translational mechanisms for academic entrepreneurship.pdf is unreadable by glob.glob. Skipping file\u001b[0m\n",
      "\u001b[92mpdf files read\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "print(\"reading pdfs in\" + str(rootDir) + \" (including subdirectories)\")\n",
    "def read_files():\n",
    "    os.chdir(rootDir)\n",
    "    for file in glob.glob(\"**/*.pdf\"):\n",
    "        try:\n",
    "            pdfFiles.append(file)\n",
    "            pdfReaders.append(PdfReader(file))\n",
    "        except:\n",
    "            print(bcolors.FAIL + \"error: \" + file + \" is unreadable by glob.glob. Skipping file\" + bcolors.ENDC)\n",
    "    print(bcolors.OKGREEN + \"pdf files read\" + bcolors.ENDC)\n",
    "    print()\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ffcb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pdfs to text files (duplicate pdfs are handled automagically)\n",
      "\u001b[91merror: \"None\" not extractable with PyPDF2, trying with pdfminer\u001b[0m\n",
      "\n",
      "\u001b[92mpdf extraction complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# extract text from pdfs to designated directory and save as txt files.\n",
    "def extract_to_txt():\n",
    "    print(\"Extracting pdfs to text files (duplicate pdfs are handled automagically)\")\n",
    "    os.chdir(txtExtractDir)\n",
    "    counter = 0\n",
    "    text = \"\"\n",
    "    for i in pdfReaders:\n",
    "        counter += 1\n",
    "        with open(str([i.metadata.title]) + \".txt\", 'w', encoding=\"utf-8\") as file:\n",
    "      \n",
    "            # add doc title to array for reference / tagging\n",
    "            docLabels.append(i.metadata.title)\n",
    "            try:\n",
    "                for j in range(len(i.pages)):\n",
    "                    # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                    text += i.getPage(j).extract_text()\n",
    "                    text = \"\".join(line.strip(\"\\n\") for line in text)  \n",
    "\n",
    "                \n",
    "                    \n",
    "            except Exception as exc:\n",
    "                print(bcolors.FAIL + \"error: \" + \"\\\"\" + str(i.metadata.title) + \"\\\"\" + \" not extractable with PyPDF2, trying with pdfminer\" + bcolors.ENDC)\n",
    "                print()\n",
    "                # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                text += fallback_text_extraction(rootDir + \"/\" + pdfFiles[counter])\n",
    "                text = \"\".join(line.strip(\"\\n\") for line in text)     \n",
    "                \n",
    " \n",
    "            file.write(text)\n",
    "    print(bcolors.OKGREEN + \"pdf extraction complete\" + bcolors.ENDC)\n",
    "extract_to_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1264dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a training corpus from all txt files found in designated directory\n",
    "class CorpusGen(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self, tokens_only=False):\n",
    "        counter = 0\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            \n",
    "            with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    tokens = gensim.utils.simple_preprocess(line, min_len=3, max_len=20, deacc=True)\n",
    "                    if tokens_only:\n",
    "                        yield tokens\n",
    "                    else:\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(tokens, [counter])\n",
    "            counter += 1\n",
    "        \n",
    "trainCorpus = list(CorpusGen('/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored/txt-extractions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d4ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:34:40,353 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>', 'datetime': '2022-09-28T17:34:40.353871', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2022-09-28 17:34:40,354 : INFO : collecting all words and their counts\n",
      "2022-09-28 17:34:40,354 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-09-28 17:34:40,408 : INFO : collected 13122 word types and 12 unique tags from a corpus of 12 examples and 614530 words\n",
      "2022-09-28 17:34:40,409 : INFO : Creating a fresh vocabulary\n",
      "2022-09-28 17:34:40,429 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 13119 unique words (99.98% of original 13122, drops 3)', 'datetime': '2022-09-28T17:34:40.429008', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:34:40,429 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 614527 word corpus (100.00% of original 614530, drops 3)', 'datetime': '2022-09-28T17:34:40.429537', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:34:40,456 : INFO : deleting the raw counts dictionary of 13122 items\n",
      "2022-09-28 17:34:40,456 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2022-09-28 17:34:40,457 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 531100.0515330511 word corpus (86.4%% of prior 614527)', 'datetime': '2022-09-28T17:34:40.457163', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:34:40,500 : INFO : estimated required memory for 13119 words and 50 dimensions: 11811900 bytes\n",
      "2022-09-28 17:34:40,501 : INFO : resetting layer weights\n",
      "2022-09-28 17:34:40,504 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 13119 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-09-28T17:34:40.504122', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-09-28 17:34:40,557 : INFO : EPOCH 0: training on 614530 raw words (104569 effective words) took 0.1s, 2001983 effective words/s\n",
      "2022-09-28 17:34:40,611 : INFO : EPOCH 1: training on 614530 raw words (104554 effective words) took 0.1s, 1988780 effective words/s\n",
      "2022-09-28 17:34:40,664 : INFO : EPOCH 2: training on 614530 raw words (104521 effective words) took 0.1s, 2052364 effective words/s\n",
      "2022-09-28 17:34:40,718 : INFO : EPOCH 3: training on 614530 raw words (104558 effective words) took 0.1s, 2032606 effective words/s\n",
      "2022-09-28 17:34:40,771 : INFO : EPOCH 4: training on 614530 raw words (104540 effective words) took 0.1s, 2057425 effective words/s\n",
      "2022-09-28 17:34:40,823 : INFO : EPOCH 5: training on 614530 raw words (104528 effective words) took 0.1s, 2075897 effective words/s\n",
      "2022-09-28 17:34:40,878 : INFO : EPOCH 6: training on 614530 raw words (104598 effective words) took 0.1s, 1997889 effective words/s\n",
      "2022-09-28 17:34:40,928 : INFO : EPOCH 7: training on 614530 raw words (104542 effective words) took 0.0s, 2187269 effective words/s\n",
      "2022-09-28 17:34:40,982 : INFO : EPOCH 8: training on 614530 raw words (104530 effective words) took 0.1s, 2019380 effective words/s\n",
      "2022-09-28 17:34:41,032 : INFO : EPOCH 9: training on 614530 raw words (104555 effective words) took 0.0s, 2150688 effective words/s\n",
      "2022-09-28 17:34:41,086 : INFO : EPOCH 10: training on 614530 raw words (104550 effective words) took 0.1s, 1983379 effective words/s\n",
      "2022-09-28 17:34:41,138 : INFO : EPOCH 11: training on 614530 raw words (104527 effective words) took 0.1s, 2046496 effective words/s\n",
      "2022-09-28 17:34:41,192 : INFO : EPOCH 12: training on 614530 raw words (104563 effective words) took 0.1s, 2041229 effective words/s\n",
      "2022-09-28 17:34:41,244 : INFO : EPOCH 13: training on 614530 raw words (104556 effective words) took 0.1s, 2028914 effective words/s\n",
      "2022-09-28 17:34:41,296 : INFO : EPOCH 14: training on 614530 raw words (104543 effective words) took 0.1s, 2081621 effective words/s\n",
      "2022-09-28 17:34:41,345 : INFO : EPOCH 15: training on 614530 raw words (104525 effective words) took 0.0s, 2246720 effective words/s\n",
      "2022-09-28 17:34:41,394 : INFO : EPOCH 16: training on 614530 raw words (104515 effective words) took 0.0s, 2200694 effective words/s\n",
      "2022-09-28 17:34:41,442 : INFO : EPOCH 17: training on 614530 raw words (104578 effective words) took 0.0s, 2233302 effective words/s\n",
      "2022-09-28 17:34:41,492 : INFO : EPOCH 18: training on 614530 raw words (104572 effective words) took 0.0s, 2211945 effective words/s\n",
      "2022-09-28 17:34:41,541 : INFO : EPOCH 19: training on 614530 raw words (104539 effective words) took 0.0s, 2186646 effective words/s\n",
      "2022-09-28 17:34:41,589 : INFO : EPOCH 20: training on 614530 raw words (104547 effective words) took 0.0s, 2204071 effective words/s\n",
      "2022-09-28 17:34:41,635 : INFO : EPOCH 21: training on 614530 raw words (104520 effective words) took 0.0s, 2324108 effective words/s\n",
      "2022-09-28 17:34:41,683 : INFO : EPOCH 22: training on 614530 raw words (104565 effective words) took 0.0s, 2286742 effective words/s\n",
      "2022-09-28 17:34:41,732 : INFO : EPOCH 23: training on 614530 raw words (104549 effective words) took 0.0s, 2246387 effective words/s\n",
      "2022-09-28 17:34:41,779 : INFO : EPOCH 24: training on 614530 raw words (104544 effective words) took 0.0s, 2313409 effective words/s\n",
      "2022-09-28 17:34:41,828 : INFO : EPOCH 25: training on 614530 raw words (104552 effective words) took 0.0s, 2150942 effective words/s\n",
      "2022-09-28 17:34:41,875 : INFO : EPOCH 26: training on 614530 raw words (104532 effective words) took 0.0s, 2358621 effective words/s\n",
      "2022-09-28 17:34:41,920 : INFO : EPOCH 27: training on 614530 raw words (104530 effective words) took 0.0s, 2416515 effective words/s\n",
      "2022-09-28 17:34:41,966 : INFO : EPOCH 28: training on 614530 raw words (104548 effective words) took 0.0s, 2326035 effective words/s\n",
      "2022-09-28 17:34:42,013 : INFO : EPOCH 29: training on 614530 raw words (104539 effective words) took 0.0s, 2325651 effective words/s\n",
      "2022-09-28 17:34:42,058 : INFO : EPOCH 30: training on 614530 raw words (104550 effective words) took 0.0s, 2404897 effective words/s\n",
      "2022-09-28 17:34:42,102 : INFO : EPOCH 31: training on 614530 raw words (104540 effective words) took 0.0s, 2395365 effective words/s\n",
      "2022-09-28 17:34:42,146 : INFO : EPOCH 32: training on 614530 raw words (104547 effective words) took 0.0s, 2492820 effective words/s\n",
      "2022-09-28 17:34:42,192 : INFO : EPOCH 33: training on 614530 raw words (104569 effective words) took 0.0s, 2373381 effective words/s\n",
      "2022-09-28 17:34:42,239 : INFO : EPOCH 34: training on 614530 raw words (104545 effective words) took 0.0s, 2340114 effective words/s\n",
      "2022-09-28 17:34:42,284 : INFO : EPOCH 35: training on 614530 raw words (104529 effective words) took 0.0s, 2360750 effective words/s\n",
      "2022-09-28 17:34:42,330 : INFO : EPOCH 36: training on 614530 raw words (104539 effective words) took 0.0s, 2324940 effective words/s\n",
      "2022-09-28 17:34:42,377 : INFO : EPOCH 37: training on 614530 raw words (104593 effective words) took 0.0s, 2294727 effective words/s\n",
      "2022-09-28 17:34:42,424 : INFO : EPOCH 38: training on 614530 raw words (104553 effective words) took 0.0s, 2328529 effective words/s\n",
      "2022-09-28 17:34:42,471 : INFO : EPOCH 39: training on 614530 raw words (104514 effective words) took 0.0s, 2344268 effective words/s\n",
      "2022-09-28 17:34:42,471 : INFO : Doc2Vec lifecycle event {'msg': 'training on 24581200 raw words (4181868 effective words) took 2.0s, 2126068 effective words/s', 'datetime': '2022-09-28T17:34:42.471433', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# establish a model and build the vocab\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(trainCorpus)\n",
    "model.train(trainCorpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefe6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:36:04,210 : WARNING : attribute doc_count not present in KeyedVectors<vector_size=50, 12 keys>; will store in internal index_to_key order\n",
      "2022-09-28 17:36:04,211 : INFO : storing 12x50 projection weights into doc_tensor.w2v\n",
      "2022-09-28 17:36:04,220 : INFO : running ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
      "2022-09-28 17:36:04,221 : INFO : loading projection weights from doc_tensor.w2v\n",
      "2022-09-28 17:36:04,223 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (12, 50) matrix of type float32 from doc_tensor.w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-09-28T17:36:04.223864', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2022-09-28 17:36:04,225 : INFO : 2D tensor file saved to pdf_plot_tensor.tsv\n",
      "2022-09-28 17:36:04,226 : INFO : Tensor metadata file saved to pdf_plot_metadata.tsv\n",
      "2022-09-28 17:36:04,226 : INFO : finished running word2vec2tensor.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAFOR Journal of Cultural Studies – Volume 6 – Issue 1 \n",
      "frvir-2022-779148 1..5\n",
      "ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users\n",
      "The effects of visual context and individual differences on perception and evaluation of modern art and graffiti art\n",
      "User attention and behaviour in virtual reality art encounter\n",
      "Microsoft Word - 48710116.DOC\n",
      "pone.0099019 1..8\n",
      "Making Art Therapy Virtual: Integrating Virtual Reality Into Art Therapy With Adolescents\n",
      "Microsoft Word - CHI2018_LucidDreaming_v5.docx\n",
      "g5grap.lo\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate and format data files for tensorboard visualisation\n",
    "os.chdir(modelDataDir)\n",
    "model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)\n",
    "%run ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
    "\n",
    "text = \"\"    \n",
    "with open('pdf_plot_metadata.tsv', 'w') as file:\n",
    "    file.write('title\\n')\n",
    "    for fname in os.listdir(txtExtractDir):\n",
    "        if fname.endswith('.txt'):\n",
    "            text = fname\n",
    "            text = re.sub('\\[\\'', '', text)\n",
    "            text = re.sub('\\'\\].txt', '', text)\n",
    "            text = re.sub('\\[', '', text)\n",
    "            text = re.sub('\\].txt', '', text)     \n",
    "            print(text)\n",
    "            file.write(\"%s\\n\" % text)\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "958a3c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"internet\" appears this many times in corpus:\n",
      "{102}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word occurence check\n",
    "checkWord = \"internet\"\n",
    "print(\"\\\"\" + str(checkWord) + \"\\\"\" + \" appears this many times in corpus:\")\n",
    "print({model.wv.get_vecattr(checkWord, 'count')})\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c98552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infering a default vector\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# infer a vector from corupus (I dont actually know (yet) what this means or does! :D )\n",
    "print(\"infering a default vector\")\n",
    "print()\n",
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d5118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire corpus to a txt file\n",
    "with open(modelDataDir + \"/\" + \"train-corpus.txt\", 'w') as file:\n",
    "    file.write(str(trainCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b802f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessing the model (this can take a while)\n",
      "\u001b[92mmodel assessed\u001b[0m\n",
      "\n",
      "Counter({0: 3, 3: 3, 8: 1, 7: 1, 9: 1, 5: 1, 6: 1, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "# assessing the model\n",
    "print(\"assessing the model (this can take a while)\")\n",
    "ranks = []\n",
    "secondRanks = []\n",
    "for doc_id in range(len(trainCorpus)):\n",
    "        inferredVector = model.infer_vector(trainCorpus[doc_id].words)\n",
    "        sims = model.dv.most_similar([inferredVector], topn=len(model.dv))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        secondRanks.append(sims[1])\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(bcolors.OKGREEN + \"model assessed, all is well in computer land\" + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12276770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabd240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

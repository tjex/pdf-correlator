{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os, glob, re, io, random, gensim, smart_open, logging, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as fallback_text_extraction\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pdfReaders = []\n",
    "pdfFiles = []\n",
    "docLabels = []\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootDir = \"/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored\"\n",
    "txtExtractDir = rootDir + '/txt-extractions'\n",
    "modelDataDir = rootDir + '/model-data'\n",
    "zoteroDir = '/Users/tillman/t-root/zotero/storage'\n",
    "\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61af71d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pdfs in/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored (including subdirectories)\n",
      "\u001b[91merror: pdf-tests/Simeone et al_2018_Arts and design as translational mechanisms for academic entrepreneurship.pdf is unreadable by glob.glob. Skipping file\u001b[0m\n",
      "\u001b[92mpdf files read\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "print(\"reading pdfs in\" + str(rootDir) + \" (including subdirectories)\")\n",
    "def read_files():\n",
    "    os.chdir(rootDir)\n",
    "    for file in glob.glob(\"**/*.pdf\"):\n",
    "        try:\n",
    "            pdfFiles.append(file)\n",
    "            pdfReaders.append(PdfReader(file))\n",
    "        except:\n",
    "            print(bcolors.FAIL + \"error: \" + file + \" is unreadable by glob.glob. Skipping file\" + bcolors.ENDC)\n",
    "    print(bcolors.OKGREEN + \"pdf files read\" + bcolors.ENDC)\n",
    "    print()\n",
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b7ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pdfs to text files (duplicate pdfs are handled automagically)\n",
      "\u001b[91merror: \"None\" not extractable with PyPDF2, trying with pdfminer\u001b[0m\n",
      "\n",
      "\u001b[92mpdf extraction complete\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# extract text from pdfs to designated directory and save as txt files.\n",
    "def extract_to_txt():\n",
    "    print(\"Extracting pdfs to text files (duplicate pdfs are handled automagically)\")\n",
    "    os.chdir(txtExtractDir)\n",
    "    counter = 0\n",
    "    text = \"\"\n",
    "    for i in pdfReaders:\n",
    "        counter += 1\n",
    "        with open(str([i.metadata.title]) + \".txt\", 'w', encoding=\"utf-8\") as file:\n",
    "      \n",
    "            # add doc title to array for reference / tagging\n",
    "            docLabels.append(i.metadata.title)\n",
    "            try:\n",
    "                for j in range(len(i.pages)):\n",
    "                    # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                    text += i.getPage(j).extract_text()\n",
    "                    text = \"\".join(line.strip(\"\\n\") for line in text)  \n",
    "\n",
    "                \n",
    "                    \n",
    "            except Exception as exc:\n",
    "                print(bcolors.FAIL + \"error: \" + \"\\\"\" + str(i.metadata.title) + \"\\\"\" + \" not extractable with PyPDF2, trying with pdfminer\" + bcolors.ENDC)\n",
    "                print()\n",
    "                # format txt file so that each document is one one line (doc2vec requirement)\n",
    "                text += fallback_text_extraction(rootDir + \"/\" + pdfFiles[counter])\n",
    "                text = \"\".join(line.strip(\"\\n\") for line in text)     \n",
    "                \n",
    " \n",
    "            file.write(text)\n",
    "    print(bcolors.OKGREEN + \"pdf extraction complete\" + bcolors.ENDC)\n",
    "extract_to_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6794d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a training corpus from all txt files found in designated directory\n",
    "class CorpusGen(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self, tokens_only=False):\n",
    "        counter = 0\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            \n",
    "            with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    tokens = gensim.utils.simple_preprocess(line, min_len=3, max_len=20, deacc=True)\n",
    "                    if tokens_only:\n",
    "                        yield tokens\n",
    "                    else:\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(tokens, [counter])\n",
    "            counter += 1\n",
    "        \n",
    "trainCorpus = list(CorpusGen('/Users/tillman/t-root/dev/projects/2022/pdf-correlator/gitignored/txt-extractions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc17eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:11:30,803 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>', 'datetime': '2022-09-28T17:11:30.803933', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2022-09-28 17:11:30,804 : INFO : collecting all words and their counts\n",
      "2022-09-28 17:11:30,804 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-09-28 17:11:31,019 : INFO : collected 13159 word types and 13 unique tags from a corpus of 13 examples and 2452726 words\n",
      "2022-09-28 17:11:31,019 : INFO : Creating a fresh vocabulary\n",
      "2022-09-28 17:11:31,040 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 13159 unique words (100.00% of original 13159, drops 0)', 'datetime': '2022-09-28T17:11:31.040369', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:11:31,041 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2452726 word corpus (100.00% of original 2452726, drops 0)', 'datetime': '2022-09-28T17:11:31.041080', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:11:31,068 : INFO : deleting the raw counts dictionary of 13159 items\n",
      "2022-09-28 17:11:31,068 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2022-09-28 17:11:31,068 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2118646.5228541954 word corpus (86.4%% of prior 2452726)', 'datetime': '2022-09-28T17:11:31.068897', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-28 17:11:31,112 : INFO : estimated required memory for 13159 words and 50 dimensions: 11848300 bytes\n",
      "2022-09-28 17:11:31,113 : INFO : resetting layer weights\n",
      "2022-09-28 17:11:31,115 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 13159 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-09-28T17:11:31.115676', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2022-09-28 17:11:31,171 : INFO : EPOCH 0: training on 2452726 raw words (114582 effective words) took 0.1s, 2096789 effective words/s\n",
      "2022-09-28 17:11:31,224 : INFO : EPOCH 1: training on 2452726 raw words (114582 effective words) took 0.1s, 2203668 effective words/s\n",
      "2022-09-28 17:11:31,277 : INFO : EPOCH 2: training on 2452726 raw words (114540 effective words) took 0.1s, 2217061 effective words/s\n",
      "2022-09-28 17:11:31,332 : INFO : EPOCH 3: training on 2452726 raw words (114614 effective words) took 0.1s, 2200279 effective words/s\n",
      "2022-09-28 17:11:31,385 : INFO : EPOCH 4: training on 2452726 raw words (114550 effective words) took 0.1s, 2175458 effective words/s\n",
      "2022-09-28 17:11:31,439 : INFO : EPOCH 5: training on 2452726 raw words (114570 effective words) took 0.1s, 2162711 effective words/s\n",
      "2022-09-28 17:11:31,495 : INFO : EPOCH 6: training on 2452726 raw words (114566 effective words) took 0.1s, 2129118 effective words/s\n",
      "2022-09-28 17:11:31,547 : INFO : EPOCH 7: training on 2452726 raw words (114579 effective words) took 0.1s, 2267338 effective words/s\n",
      "2022-09-28 17:11:31,601 : INFO : EPOCH 8: training on 2452726 raw words (114556 effective words) took 0.1s, 2217401 effective words/s\n",
      "2022-09-28 17:11:31,655 : INFO : EPOCH 9: training on 2452726 raw words (114555 effective words) took 0.1s, 2141427 effective words/s\n",
      "2022-09-28 17:11:31,708 : INFO : EPOCH 10: training on 2452726 raw words (114501 effective words) took 0.1s, 2246210 effective words/s\n",
      "2022-09-28 17:11:31,761 : INFO : EPOCH 11: training on 2452726 raw words (114554 effective words) took 0.1s, 2272570 effective words/s\n",
      "2022-09-28 17:11:31,812 : INFO : EPOCH 12: training on 2452726 raw words (114555 effective words) took 0.1s, 2284200 effective words/s\n",
      "2022-09-28 17:11:31,861 : INFO : EPOCH 13: training on 2452726 raw words (114551 effective words) took 0.0s, 2414391 effective words/s\n",
      "2022-09-28 17:11:31,913 : INFO : EPOCH 14: training on 2452726 raw words (114561 effective words) took 0.0s, 2316167 effective words/s\n",
      "2022-09-28 17:11:31,965 : INFO : EPOCH 15: training on 2452726 raw words (114568 effective words) took 0.0s, 2306061 effective words/s\n",
      "2022-09-28 17:11:32,016 : INFO : EPOCH 16: training on 2452726 raw words (114528 effective words) took 0.0s, 2313541 effective words/s\n",
      "2022-09-28 17:11:32,064 : INFO : EPOCH 17: training on 2452726 raw words (114587 effective words) took 0.0s, 2518093 effective words/s\n",
      "2022-09-28 17:11:32,114 : INFO : EPOCH 18: training on 2452726 raw words (114526 effective words) took 0.0s, 2387797 effective words/s\n",
      "2022-09-28 17:11:32,164 : INFO : EPOCH 19: training on 2452726 raw words (114539 effective words) took 0.0s, 2351037 effective words/s\n",
      "2022-09-28 17:11:32,211 : INFO : EPOCH 20: training on 2452726 raw words (114528 effective words) took 0.0s, 2528237 effective words/s\n",
      "2022-09-28 17:11:32,260 : INFO : EPOCH 21: training on 2452726 raw words (114561 effective words) took 0.0s, 2389874 effective words/s\n",
      "2022-09-28 17:11:32,310 : INFO : EPOCH 22: training on 2452726 raw words (114551 effective words) took 0.0s, 2425051 effective words/s\n",
      "2022-09-28 17:11:32,358 : INFO : EPOCH 23: training on 2452726 raw words (114537 effective words) took 0.0s, 2484336 effective words/s\n",
      "2022-09-28 17:11:32,404 : INFO : EPOCH 24: training on 2452726 raw words (114553 effective words) took 0.0s, 2570850 effective words/s\n",
      "2022-09-28 17:11:32,453 : INFO : EPOCH 25: training on 2452726 raw words (114566 effective words) took 0.0s, 2442438 effective words/s\n",
      "2022-09-28 17:11:32,499 : INFO : EPOCH 26: training on 2452726 raw words (114570 effective words) took 0.0s, 2641934 effective words/s\n",
      "2022-09-28 17:11:32,546 : INFO : EPOCH 27: training on 2452726 raw words (114543 effective words) took 0.0s, 2529942 effective words/s\n",
      "2022-09-28 17:11:32,592 : INFO : EPOCH 28: training on 2452726 raw words (114556 effective words) took 0.0s, 2604342 effective words/s\n",
      "2022-09-28 17:11:32,638 : INFO : EPOCH 29: training on 2452726 raw words (114530 effective words) took 0.0s, 2647320 effective words/s\n",
      "2022-09-28 17:11:32,684 : INFO : EPOCH 30: training on 2452726 raw words (114549 effective words) took 0.0s, 2596127 effective words/s\n",
      "2022-09-28 17:11:32,730 : INFO : EPOCH 31: training on 2452726 raw words (114562 effective words) took 0.0s, 2587400 effective words/s\n",
      "2022-09-28 17:11:32,777 : INFO : EPOCH 32: training on 2452726 raw words (114559 effective words) took 0.0s, 2519134 effective words/s\n",
      "2022-09-28 17:11:32,826 : INFO : EPOCH 33: training on 2452726 raw words (114559 effective words) took 0.0s, 2472752 effective words/s\n",
      "2022-09-28 17:11:32,871 : INFO : EPOCH 34: training on 2452726 raw words (114533 effective words) took 0.0s, 2589155 effective words/s\n",
      "2022-09-28 17:11:32,920 : INFO : EPOCH 35: training on 2452726 raw words (114555 effective words) took 0.0s, 2443069 effective words/s\n",
      "2022-09-28 17:11:32,967 : INFO : EPOCH 36: training on 2452726 raw words (114542 effective words) took 0.0s, 2501197 effective words/s\n",
      "2022-09-28 17:11:33,015 : INFO : EPOCH 37: training on 2452726 raw words (114536 effective words) took 0.0s, 2392461 effective words/s\n",
      "2022-09-28 17:11:33,062 : INFO : EPOCH 38: training on 2452726 raw words (114535 effective words) took 0.0s, 2492361 effective words/s\n",
      "2022-09-28 17:11:33,109 : INFO : EPOCH 39: training on 2452726 raw words (114560 effective words) took 0.0s, 2538470 effective words/s\n",
      "2022-09-28 17:11:33,109 : INFO : Doc2Vec lifecycle event {'msg': 'training on 98109040 raw words (4582149 effective words) took 2.0s, 2298619 effective words/s', 'datetime': '2022-09-28T17:11:33.109411', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# establish a model and build the vocab\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(trainCorpus)\n",
    "model.train(trainCorpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1107fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:28:41,103 : WARNING : attribute doc_count not present in KeyedVectors<vector_size=50, 13 keys>; will store in internal index_to_key order\n",
      "2022-09-28 17:28:41,104 : INFO : storing 13x50 projection weights into doc_tensor.w2v\n",
      "2022-09-28 17:28:41,112 : INFO : running ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
      "2022-09-28 17:28:41,113 : INFO : loading projection weights from doc_tensor.w2v\n",
      "2022-09-28 17:28:41,117 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (13, 50) matrix of type float32 from doc_tensor.w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-09-28T17:28:41.117264', 'gensim': '4.2.0', 'python': '3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]', 'platform': 'macOS-12.0.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2022-09-28 17:28:41,119 : INFO : 2D tensor file saved to pdf_plot_tensor.tsv\n",
      "2022-09-28 17:28:41,119 : INFO : Tensor metadata file saved to pdf_plot_metadata.tsv\n",
      "2022-09-28 17:28:41,119 : INFO : finished running word2vec2tensor.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAFOR Journal of Cultural Studies – Volume 6 – Issue 1 \n",
      "frvir-2022-779148 1..5\n",
      "ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users\n",
      "The effects of visual context and individual differences on perception and evaluation of modern art and graffiti art\n",
      "User attention and behaviour in virtual reality art encounter\n",
      "Microsoft Word - 48710116.DOC\n",
      "pone.0099019 1..8\n",
      "Making Art Therapy Virtual: Integrating Virtual Reality Into Art Therapy With Adolescents\n",
      "Microsoft Word - CHI2018_LucidDreaming_v5.docx\n",
      "g5grap.lo\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate and format data files for tensorboard visualisation\n",
    "os.chdir(modelDataDir)\n",
    "model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False)\n",
    "%run ../scripts/word2vec2tensor.py -i doc_tensor.w2v -o pdf_plot\n",
    "\n",
    "text = \"\"    \n",
    "with open('pdf_plot_metadata.tsv', 'w') as file:\n",
    "    file.write('title\\n')\n",
    "    for fname in os.listdir(txtExtractDir):\n",
    "        if fname.endswith('.txt'):\n",
    "            text = fname\n",
    "            text = re.sub('\\[\\'', '', text)\n",
    "            text = re.sub('\\'\\].txt', '', text)\n",
    "            text = re.sub('\\[', '', text)\n",
    "            text = re.sub('\\].txt', '', text)     \n",
    "            print(text)\n",
    "            file.write(\"%s\\n\" % text)\n",
    "        else:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edceaeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\" appears this many times in corpus:\n",
      "{162592}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word occurence check\n",
    "checkWord = \"the\"\n",
    "print(\"\\\"\" + str(checkWord) + \"\\\"\" + \" appears this many times in corpus:\")\n",
    "print({model.wv.get_vecattr(checkWord, 'count')})\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8dd148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infering a default vector\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# infer a vector from corupus (I dont actually know (yet) what this means or does! :D )\n",
    "print(\"infering a default vector\")\n",
    "print()\n",
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67952436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire corpus to a txt file\n",
    "with open(modelDataDir + \"/\" + \"train-corpus.txt\", 'w') as file:\n",
    "    file.write(str(trainCorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b75c406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessing the model (this can take a while)\n",
      "\u001b[92mmodel assessed\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assessing the model\n",
    "print(\"assessing the model (this can take a while)\")\n",
    "ranks = []\n",
    "secondRanks = []\n",
    "for doc_id in range(len(trainCorpus)):\n",
    "        inferredVector = model.infer_vector(trainCorpus[doc_id].words)\n",
    "        sims = model.dv.most_similar([inferredVector], topn=len(model.dv))\n",
    "        rank = [docid for docid, sim in sims].index(doc_id)\n",
    "        ranks.append(rank)\n",
    "        secondRanks.append(sims[1])\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(bcolors.OKGREEN + \"model assessed\" + bcolors.ENDC)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a715004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcea68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
